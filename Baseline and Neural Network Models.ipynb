{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cyberbullying Auto Detection\n",
    "### Baseline Model and Neural Networks\n",
    "*Wenqu Wang, Casey Yoon*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy, TensorFlow, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow import keras\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, Dropout\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['graph',\n",
       " 'twitter_sentiment.ipynb',\n",
       " 'twitter_racism_parsed_dataset.csv',\n",
       " '.git',\n",
       " 'twitter_parsed_dataset.csv',\n",
       " 'Untitled.ipynb',\n",
       " '.ipynb_checkpoints',\n",
       " 'graph.png',\n",
       " 'twitter_sexism_parsed_dataset.csv']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>Text</th>\n",
       "      <th>Annotation</th>\n",
       "      <th>oh_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.74948705591165E+017</td>\n",
       "      <td>5.74948705591165E+017</td>\n",
       "      <td>@halalflaws @biebervalue @greenlinerzjm I read...</td>\n",
       "      <td>none</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.71917888690393E+017</td>\n",
       "      <td>5.71917888690393E+017</td>\n",
       "      <td>@ShreyaBafna3 Now you idiots claim that people...</td>\n",
       "      <td>none</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.90255841338601E+017</td>\n",
       "      <td>3.90255841338601E+017</td>\n",
       "      <td>RT @Mooseoftorment Call me sexist, but when I ...</td>\n",
       "      <td>sexism</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.68208850655916E+017</td>\n",
       "      <td>5.68208850655916E+017</td>\n",
       "      <td>@g0ssipsquirrelx Wrong, ISIS follows the examp...</td>\n",
       "      <td>racism</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.75596338802373E+017</td>\n",
       "      <td>5.75596338802373E+017</td>\n",
       "      <td>#mkr No No No No No No</td>\n",
       "      <td>none</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   index                     id  \\\n",
       "0  5.74948705591165E+017  5.74948705591165E+017   \n",
       "1  5.71917888690393E+017  5.71917888690393E+017   \n",
       "2  3.90255841338601E+017  3.90255841338601E+017   \n",
       "3  5.68208850655916E+017  5.68208850655916E+017   \n",
       "4  5.75596338802373E+017  5.75596338802373E+017   \n",
       "\n",
       "                                                Text Annotation  oh_label  \n",
       "0  @halalflaws @biebervalue @greenlinerzjm I read...       none       0.0  \n",
       "1  @ShreyaBafna3 Now you idiots claim that people...       none       0.0  \n",
       "2  RT @Mooseoftorment Call me sexist, but when I ...     sexism       1.0  \n",
       "3  @g0ssipsquirrelx Wrong, ISIS follows the examp...     racism       1.0  \n",
       "4                             #mkr No No No No No No       none       0.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### We aggregate all the data into one dataframe\n",
    "\n",
    "parsed = pd.read_csv('twitter_parsed_dataset.csv')\n",
    "racism = pd.read_csv('twitter_racism_parsed_dataset.csv')\n",
    "sexism = pd.read_csv('twitter_sexism_parsed_dataset.csv')\n",
    "\n",
    "twitter_data = pd.concat([parsed, racism, sexism]).dropna()\n",
    "twitter_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>Text</th>\n",
       "      <th>Annotation</th>\n",
       "      <th>oh_label</th>\n",
       "      <th>tokenized_tweets</th>\n",
       "      <th>cleaned_tweets</th>\n",
       "      <th>num_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.74948705591165E+017</td>\n",
       "      <td>5.74948705591165E+017</td>\n",
       "      <td>@halalflaws @biebervalue @greenlinerzjm I read...</td>\n",
       "      <td>none</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[i, read, them, in, context, no, change, in, m...</td>\n",
       "      <td>i read them in context no change in meaning th...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.71917888690393E+017</td>\n",
       "      <td>5.71917888690393E+017</td>\n",
       "      <td>@ShreyaBafna3 Now you idiots claim that people...</td>\n",
       "      <td>none</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[now, you, idiots, claim, that, people, who, t...</td>\n",
       "      <td>now you idiots claim that people who tried to ...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.90255841338601E+017</td>\n",
       "      <td>3.90255841338601E+017</td>\n",
       "      <td>RT @Mooseoftorment Call me sexist, but when I ...</td>\n",
       "      <td>sexism</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[rt, call, me, sexist, but, when, i, go, to, a...</td>\n",
       "      <td>rt call me sexist but when i go to an auto pla...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.68208850655916E+017</td>\n",
       "      <td>5.68208850655916E+017</td>\n",
       "      <td>@g0ssipsquirrelx Wrong, ISIS follows the examp...</td>\n",
       "      <td>racism</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[wrong, isis, follows, the, example, of, moham...</td>\n",
       "      <td>wrong isis follows the example of mohammed and...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.75596338802373E+017</td>\n",
       "      <td>5.75596338802373E+017</td>\n",
       "      <td>#mkr No No No No No No</td>\n",
       "      <td>none</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[#mkr, no, no, no, no, no, no]</td>\n",
       "      <td>#mkr no no no no no no</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   index                     id  \\\n",
       "0  5.74948705591165E+017  5.74948705591165E+017   \n",
       "1  5.71917888690393E+017  5.71917888690393E+017   \n",
       "2  3.90255841338601E+017  3.90255841338601E+017   \n",
       "3  5.68208850655916E+017  5.68208850655916E+017   \n",
       "4  5.75596338802373E+017  5.75596338802373E+017   \n",
       "\n",
       "                                                Text Annotation  oh_label  \\\n",
       "0  @halalflaws @biebervalue @greenlinerzjm I read...       none       0.0   \n",
       "1  @ShreyaBafna3 Now you idiots claim that people...       none       0.0   \n",
       "2  RT @Mooseoftorment Call me sexist, but when I ...     sexism       1.0   \n",
       "3  @g0ssipsquirrelx Wrong, ISIS follows the examp...     racism       1.0   \n",
       "4                             #mkr No No No No No No       none       0.0   \n",
       "\n",
       "                                    tokenized_tweets  \\\n",
       "0  [i, read, them, in, context, no, change, in, m...   \n",
       "1  [now, you, idiots, claim, that, people, who, t...   \n",
       "2  [rt, call, me, sexist, but, when, i, go, to, a...   \n",
       "3  [wrong, isis, follows, the, example, of, moham...   \n",
       "4                     [#mkr, no, no, no, no, no, no]   \n",
       "\n",
       "                                      cleaned_tweets  num_tokens  \n",
       "0  i read them in context no change in meaning th...          18  \n",
       "1  now you idiots claim that people who tried to ...          22  \n",
       "2  rt call me sexist but when i go to an auto pla...          19  \n",
       "3  wrong isis follows the example of mohammed and...          11  \n",
       "4                             #mkr no no no no no no           7  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def cleaning_tweets(tweet):\n",
    "    # 1. Remove Twitter handles (@user)\n",
    "    users = re.findall(\"@[\\w]*\", tweet) # tokenizing\n",
    "    for user in users:\n",
    "        tweet = re.sub(user, '', tweet)\n",
    "\n",
    "    # 2. Remove, Punctuations, Numbers, and Special Characters (keep hashtags)\n",
    "    tweet = re.sub(\"[^a-zA-Z#]\", \" \", tweet)\n",
    "\n",
    "    # 3. Lowercase all\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    # 4. Splitting text into tokens\n",
    "    tweet = tweet.split()\n",
    "    \n",
    "    return tweet\n",
    "\n",
    "\n",
    "twitter_data['tokenized_tweets'] = twitter_data['Text'].apply(cleaning_tweets)\n",
    "twitter_data['cleaned_tweets'] = twitter_data['tokenized_tweets'].apply(lambda x: ' '.join(x))\n",
    "twitter_data['num_tokens'] = twitter_data['tokenized_tweets'].apply(len)\n",
    "twitter_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9327     there is such a diff between reality amp what ...\n",
       "14633    katie s a fatty model hahahaha #mkr #killerblo...\n",
       "4197     it is really funny all the assumptions they ma...\n",
       "3534                     origin is a flaming piece of shit\n",
       "4500     no you don t i thought of a really funny joke ...\n",
       "Name: cleaned_tweets, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(twitter_data['cleaned_tweets'], twitter_data['oh_label'], test_size=0.20, random_state=42)\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9327     0.0\n",
       "14633    1.0\n",
       "4197     0.0\n",
       "3534     0.0\n",
       "4500     1.0\n",
       "Name: oh_label, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9040"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize data using Tf-idf\n",
    "vec = TfidfVectorizer(max_features=500)\n",
    "X_vectrain = vec.fit_transform(X_train).toarray()\n",
    "X_vectest = vec.transform(X_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7609513274336284"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Baseline accuracy, predicting all of one class.\n",
    "1 - np.mean(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score =  0.8367223652500073\n",
      "accuracy =  0.8485619469026549\n"
     ]
    }
   ],
   "source": [
    "### Logistic Regression\n",
    "clf = LogisticRegression(random_state=0).fit(X_vectrain, y_train)\n",
    "pred = clf.predict(X_vectest)\n",
    "print(\"f1_score = \", metrics.f1_score(y_test, pred, average=\"weighted\"))\n",
    "print(\"accuracy = \", metrics.accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ti-idf with Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 64)                32064     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 36,289\n",
      "Trainable params: 36,289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Build NN model\n",
    "model = Sequential()\n",
    "model.add(Dense(64,input_shape=(500,)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(64))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1130/1130 [==============================] - 2s 1ms/step - loss: 0.4114 - acc: 0.8276\n",
      "Epoch 2/10\n",
      "1130/1130 [==============================] - 2s 1ms/step - loss: 0.3372 - acc: 0.8661\n",
      "Epoch 3/10\n",
      "1130/1130 [==============================] - 2s 1ms/step - loss: 0.2865 - acc: 0.8926\n",
      "Epoch 4/10\n",
      "1130/1130 [==============================] - 2s 1ms/step - loss: 0.2333 - acc: 0.9162\n",
      "Epoch 5/10\n",
      "1130/1130 [==============================] - 2s 1ms/step - loss: 0.1923 - acc: 0.9318\n",
      "Epoch 6/10\n",
      "1130/1130 [==============================] - 2s 1ms/step - loss: 0.1630 - acc: 0.9434\n",
      "Epoch 7/10\n",
      "1130/1130 [==============================] - 2s 1ms/step - loss: 0.1427 - acc: 0.9510\n",
      "Epoch 8/10\n",
      "1130/1130 [==============================] - 2s 1ms/step - loss: 0.1254 - acc: 0.9559\n",
      "Epoch 9/10\n",
      "1130/1130 [==============================] - 2s 1ms/step - loss: 0.1113 - acc: 0.9612\n",
      "Epoch 10/10\n",
      "1130/1130 [==============================] - 2s 1ms/step - loss: 0.1003 - acc: 0.9655\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f3cd4a23c10>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_vectrain,y_train,batch_size=32,epochs=10,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-13-f0a5dcd3f01c>:1: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict_classes(X_vectest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score =  0.953318747352476\n",
      "accuracy =  0.9542035398230089\n"
     ]
    }
   ],
   "source": [
    "print(\"f1_score = \", metrics.f1_score(y_test, pred, average=\"weighted\"))\n",
    "print(\"accuracy = \", metrics.accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pred.reshape((-1,))\n",
    "a = pd.DataFrame(X_test[pred!=y_test])\n",
    "indices = [i for i in a.index]\n",
    "misclassified = twitter_data.iloc[indices,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [i for i in a.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>oh_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1150</th>\n",
       "      <td>#mkr I think I just threw up in my mouth :(</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3021</th>\n",
       "      <td>So dissapointed Kat &amp;amp; Andre weren't elimin...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1839</th>\n",
       "      <td>RT @swagd0ctor: @VileIslam @TRobinsonNewEra @O...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7085</th>\n",
       "      <td>RT @DTNIraq: DTN Iraq: Iraq vows Tikrit victor...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1113</th>\n",
       "      <td>@Alfonso_AraujoG @ardiem1m @MaxBlumenthal @old...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11503</th>\n",
       "      <td>@iFalasteen No, this is what Muslim brutality ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12598</th>\n",
       "      <td>@0xabad1dea unless you are the help desk perso...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6336</th>\n",
       "      <td>Bye bye basic bitches 👋 #mkr</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2408</th>\n",
       "      <td>RT @Amoka: Video supposedly showing Shia milit...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11982</th>\n",
       "      <td>@mary__kaye she's always hideous! #mkr</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7966</th>\n",
       "      <td>@grexican Free speech, science, AND evolution!...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8281</th>\n",
       "      <td>did they end up making about 5 gozlemes in tot...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12748</th>\n",
       "      <td>@HenryBeans Sexism \"out of context\" = sexism. ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5811</th>\n",
       "      <td>Stop calling it \"being strategic\" when really ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13422</th>\n",
       "      <td>@NewEraJihadi @ShamiAnalyst Who needs a propag...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2525</th>\n",
       "      <td>@thespypoet Prison is the same...except with m...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8496</th>\n",
       "      <td>Lip reading coming in handy! #mkr #thatsfunny</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3826</th>\n",
       "      <td>Don't know about you but I'm soooo over so cal...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6008</th>\n",
       "      <td>@nwOryzen @_DirtyTruths Here is report includi...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5660</th>\n",
       "      <td>@vex0rian @m1sp consume_all vs consume-all, do...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text  oh_label\n",
       "1150         #mkr I think I just threw up in my mouth :(       0.0\n",
       "3021   So dissapointed Kat &amp; Andre weren't elimin...       0.0\n",
       "1839   RT @swagd0ctor: @VileIslam @TRobinsonNewEra @O...       1.0\n",
       "7085   RT @DTNIraq: DTN Iraq: Iraq vows Tikrit victor...       0.0\n",
       "1113   @Alfonso_AraujoG @ardiem1m @MaxBlumenthal @old...       1.0\n",
       "11503  @iFalasteen No, this is what Muslim brutality ...       0.0\n",
       "12598  @0xabad1dea unless you are the help desk perso...       0.0\n",
       "6336                        Bye bye basic bitches 👋 #mkr       1.0\n",
       "2408   RT @Amoka: Video supposedly showing Shia milit...       1.0\n",
       "11982             @mary__kaye she's always hideous! #mkr       1.0\n",
       "7966   @grexican Free speech, science, AND evolution!...       0.0\n",
       "8281   did they end up making about 5 gozlemes in tot...       0.0\n",
       "12748  @HenryBeans Sexism \"out of context\" = sexism. ...       0.0\n",
       "5811   Stop calling it \"being strategic\" when really ...       0.0\n",
       "13422  @NewEraJihadi @ShamiAnalyst Who needs a propag...       0.0\n",
       "2525   @thespypoet Prison is the same...except with m...       1.0\n",
       "8496       Lip reading coming in handy! #mkr #thatsfunny       0.0\n",
       "3826   Don't know about you but I'm soooo over so cal...       0.0\n",
       "6008   @nwOryzen @_DirtyTruths Here is report includi...       0.0\n",
       "5660   @vex0rian @m1sp consume_all vs consume-all, do...       0.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "misclassified = twitter_data.iloc[indices,[2,4]]\n",
    "misclassified.tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN + Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19645"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Build Word2Vec\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "w2v = Word2Vec(twitter_data['tokenized_tweets'],\n",
    "               size = 200,\n",
    "               window = 5,\n",
    "               min_count = 2,\n",
    "               sg = 1,\n",
    "               hs = 0,\n",
    "               negative = 10,\n",
    "               workers = 32,\n",
    "               seed = 1)\n",
    "\n",
    "len(w2v.wv.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19645 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "for w in w2v.wv.vocab.keys():\n",
    "    embeddings_index[w] = w2v.wv[w]\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19013"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=100000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "sequences = tokenizer.texts_to_sequences(X_train)\n",
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest sentence has length: 33\n"
     ]
    }
   ],
   "source": [
    "length = []\n",
    "for x in X_train:\n",
    "    length.append(len(x.split()))\n",
    "print(\"The longest sentence has length:\",max(length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_seq = pad_sequences(sequences, maxlen=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
    "x_test_seq = pad_sequences(sequences_test, maxlen=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build embedding matrix\n",
    "num_words = 20000\n",
    "embedding_matrix = np.zeros((num_words, 200))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i >= num_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is such a diff between reality amp what certain contestants think of their cooking #mkr\n",
      "katie s a fatty model hahahaha #mkr #killerblondes\n",
      "it is really funny all the assumptions they make about how it works and how much they are wrong\n",
      "origin is a flaming piece of shit\n",
      "no you don t i thought of a really funny joke and i promise i m not sexist but i have to say it\n"
     ]
    }
   ],
   "source": [
    "# print first 5 sentences\n",
    "for x in X_train[:5]:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[52, 8, 283, 6, 2357, 373, 622, 51, 36, 816, 533, 72, 9, 65, 177, 3],\n",
       " [228, 17, 6, 12235, 983, 1633, 3, 442],\n",
       " [13,\n",
       "  8,\n",
       "  84,\n",
       "  297,\n",
       "  39,\n",
       "  1,\n",
       "  3592,\n",
       "  21,\n",
       "  96,\n",
       "  38,\n",
       "  56,\n",
       "  13,\n",
       "  768,\n",
       "  7,\n",
       "  56,\n",
       "  121,\n",
       "  21,\n",
       "  18,\n",
       "  226],\n",
       " [3372, 8, 6, 8063, 681, 9, 134],\n",
       " [34,\n",
       "  11,\n",
       "  44,\n",
       "  5,\n",
       "  2,\n",
       "  257,\n",
       "  9,\n",
       "  6,\n",
       "  84,\n",
       "  297,\n",
       "  458,\n",
       "  7,\n",
       "  2,\n",
       "  2196,\n",
       "  2,\n",
       "  27,\n",
       "  19,\n",
       "  45,\n",
       "  22,\n",
       "  2,\n",
       "  26,\n",
       "  4,\n",
       "  116,\n",
       "  13]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first five sentences with embeddings\n",
    "sequences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure the embedding matches the word\n",
    "np.array_equal(embedding_matrix[8] ,embeddings_index.get('is'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 40, 200)           4000000   \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 36, 128)           128128    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 4,129,429\n",
      "Trainable params: 4,129,429\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#build CNN model\n",
    "model = Sequential()\n",
    "embedding_dim = 5\n",
    "model.add(layers.Embedding(20000, 200, weights=[embedding_matrix], input_length=40, trainable=True))\n",
    "model.add(layers.Conv1D(128, 5, padding='valid',activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=opt,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1130/1130 [==============================] - 83s 74ms/step - loss: 0.3460 - acc: 0.8547\n",
      "Epoch 2/10\n",
      "1130/1130 [==============================] - 83s 73ms/step - loss: 0.1587 - acc: 0.9426\n",
      "Epoch 3/10\n",
      "1130/1130 [==============================] - 83s 73ms/step - loss: 0.0626 - acc: 0.9784\n",
      "Epoch 4/10\n",
      "1130/1130 [==============================] - 83s 73ms/step - loss: 0.0363 - acc: 0.9882\n",
      "Epoch 5/10\n",
      "1130/1130 [==============================] - 83s 73ms/step - loss: 0.0264 - acc: 0.9911\n",
      "Epoch 6/10\n",
      "1130/1130 [==============================] - 82s 73ms/step - loss: 0.0207 - acc: 0.9932\n",
      "Epoch 7/10\n",
      "1130/1130 [==============================] - 82s 73ms/step - loss: 0.0163 - acc: 0.9939\n",
      "Epoch 8/10\n",
      "1130/1130 [==============================] - 83s 73ms/step - loss: 0.0153 - acc: 0.9941\n",
      "Epoch 9/10\n",
      "1130/1130 [==============================] - 83s 73ms/step - loss: 0.0129 - acc: 0.9955\n",
      "Epoch 10/10\n",
      "1130/1130 [==============================] - 83s 73ms/step - loss: 0.0122 - acc: 0.9959\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f3cd4a23be0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train_seq,y_train,batch_size=32,epochs=10,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score =  0.9802840691056793\n",
      "accuracy =  0.9804203539823009\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict_classes(x_test_seq)\n",
    "print(\"f1_score = \", metrics.f1_score(y_test, pred, average=\"weighted\"))\n",
    "print(\"accuracy = \", metrics.accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>Text</th>\n",
       "      <th>Annotation</th>\n",
       "      <th>oh_label</th>\n",
       "      <th>tokenized_tweets</th>\n",
       "      <th>cleaned_tweets</th>\n",
       "      <th>num_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11265</th>\n",
       "      <td>5.75596510114533E+017</td>\n",
       "      <td>5.75596510114533E+017</td>\n",
       "      <td>RT @mary__kaye: That face Kat just pulled was ...</td>\n",
       "      <td>sexism</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[rt, that, face, kat, just, pulled, was, hideo...</td>\n",
       "      <td>rt that face kat just pulled was hideous #mkr</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16732</th>\n",
       "      <td>5.53019765397017E+017</td>\n",
       "      <td>5.53019765397017E+017</td>\n",
       "      <td>@azmoderate @JoeWSJ Be a man. Stop babbling an...</td>\n",
       "      <td>sexism</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[be, a, man, stop, babbling, and, squirming, a...</td>\n",
       "      <td>be a man stop babbling and squirming and admit...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6975</th>\n",
       "      <td>5.62393877504487E+017</td>\n",
       "      <td>5.62393877504487E+017</td>\n",
       "      <td>RT @Joyce_Karam: Back to Mongols? #ISIS destro...</td>\n",
       "      <td>none</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[rt, back, to, mongols, #isis, destroys, books...</td>\n",
       "      <td>rt back to mongols #isis destroys books in mos...</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5948</th>\n",
       "      <td>5.75644734963606E+017</td>\n",
       "      <td>5.75644734963606E+017</td>\n",
       "      <td>@australiacamper Yep, same. But it's also wron...</td>\n",
       "      <td>none</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[yep, same, but, it, s, also, wrong, of, #mkr,...</td>\n",
       "      <td>yep same but it s also wrong of #mkr to let th...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8977</th>\n",
       "      <td>5.76515212511179E+017</td>\n",
       "      <td>5.76515212511179E+017</td>\n",
       "      <td>@BrownBagPantry @LaurieJWillberg I haven't see...</td>\n",
       "      <td>none</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[i, haven, t, seen, anything, that, falls, und...</td>\n",
       "      <td>i haven t seen anything that falls under the l...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2408</th>\n",
       "      <td>5.64487955855573E+017</td>\n",
       "      <td>5.64487955855573E+017</td>\n",
       "      <td>RT @Amoka: Video supposedly showing Shia milit...</td>\n",
       "      <td>racism</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[rt, video, supposedly, showing, shia, militia...</td>\n",
       "      <td>rt video supposedly showing shia militia killi...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13301</th>\n",
       "      <td>5.75957369764454E+017</td>\n",
       "      <td>5.75957369764454E+017</td>\n",
       "      <td>RT @AusPolQuestTime: But wait ...  new entries...</td>\n",
       "      <td>none</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[rt, but, wait, new, entries, from, canberra, ...</td>\n",
       "      <td>rt but wait new entries from canberra in mega ...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6312</th>\n",
       "      <td>5.73395089969435E+017</td>\n",
       "      <td>5.73395089969435E+017</td>\n",
       "      <td>@GaminGlennSeto @srhbutts Closer to 200. I wro...</td>\n",
       "      <td>none</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[closer, to, i, wrote, it, in, minutes, and, w...</td>\n",
       "      <td>closer to i wrote it in minutes and was drunk ...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3826</th>\n",
       "      <td>5.7560918512923E+017</td>\n",
       "      <td>5.7560918512923E+017</td>\n",
       "      <td>Don't know about you but I'm soooo over so cal...</td>\n",
       "      <td>none</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[don, t, know, about, you, but, i, m, soooo, o...</td>\n",
       "      <td>don t know about you but i m soooo over so cal...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5660</th>\n",
       "      <td>5.64286601203687E+017</td>\n",
       "      <td>5.64286601203687E+017</td>\n",
       "      <td>@vex0rian @m1sp consume_all vs consume-all, do...</td>\n",
       "      <td>none</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[consume, all, vs, consume, all, doesn, t, men...</td>\n",
       "      <td>consume all vs consume all doesn t mention mod...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>177 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       index                     id  \\\n",
       "11265  5.75596510114533E+017  5.75596510114533E+017   \n",
       "16732  5.53019765397017E+017  5.53019765397017E+017   \n",
       "6975   5.62393877504487E+017  5.62393877504487E+017   \n",
       "5948   5.75644734963606E+017  5.75644734963606E+017   \n",
       "8977   5.76515212511179E+017  5.76515212511179E+017   \n",
       "...                      ...                    ...   \n",
       "2408   5.64487955855573E+017  5.64487955855573E+017   \n",
       "13301  5.75957369764454E+017  5.75957369764454E+017   \n",
       "6312   5.73395089969435E+017  5.73395089969435E+017   \n",
       "3826    5.7560918512923E+017   5.7560918512923E+017   \n",
       "5660   5.64286601203687E+017  5.64286601203687E+017   \n",
       "\n",
       "                                                    Text Annotation  oh_label  \\\n",
       "11265  RT @mary__kaye: That face Kat just pulled was ...     sexism       1.0   \n",
       "16732  @azmoderate @JoeWSJ Be a man. Stop babbling an...     sexism       1.0   \n",
       "6975   RT @Joyce_Karam: Back to Mongols? #ISIS destro...       none       0.0   \n",
       "5948   @australiacamper Yep, same. But it's also wron...       none       0.0   \n",
       "8977   @BrownBagPantry @LaurieJWillberg I haven't see...       none       0.0   \n",
       "...                                                  ...        ...       ...   \n",
       "2408   RT @Amoka: Video supposedly showing Shia milit...     racism       1.0   \n",
       "13301  RT @AusPolQuestTime: But wait ...  new entries...       none       0.0   \n",
       "6312   @GaminGlennSeto @srhbutts Closer to 200. I wro...       none       0.0   \n",
       "3826   Don't know about you but I'm soooo over so cal...       none       0.0   \n",
       "5660   @vex0rian @m1sp consume_all vs consume-all, do...       none       0.0   \n",
       "\n",
       "                                        tokenized_tweets  \\\n",
       "11265  [rt, that, face, kat, just, pulled, was, hideo...   \n",
       "16732  [be, a, man, stop, babbling, and, squirming, a...   \n",
       "6975   [rt, back, to, mongols, #isis, destroys, books...   \n",
       "5948   [yep, same, but, it, s, also, wrong, of, #mkr,...   \n",
       "8977   [i, haven, t, seen, anything, that, falls, und...   \n",
       "...                                                  ...   \n",
       "2408   [rt, video, supposedly, showing, shia, militia...   \n",
       "13301  [rt, but, wait, new, entries, from, canberra, ...   \n",
       "6312   [closer, to, i, wrote, it, in, minutes, and, w...   \n",
       "3826   [don, t, know, about, you, but, i, m, soooo, o...   \n",
       "5660   [consume, all, vs, consume, all, doesn, t, men...   \n",
       "\n",
       "                                          cleaned_tweets  num_tokens  \n",
       "11265      rt that face kat just pulled was hideous #mkr           9  \n",
       "16732  be a man stop babbling and squirming and admit...          12  \n",
       "6975   rt back to mongols #isis destroys books in mos...          21  \n",
       "5948   yep same but it s also wrong of #mkr to let th...          17  \n",
       "8977   i haven t seen anything that falls under the l...          17  \n",
       "...                                                  ...         ...  \n",
       "2408   rt video supposedly showing shia militia killi...          22  \n",
       "13301  rt but wait new entries from canberra in mega ...          17  \n",
       "6312   closer to i wrote it in minutes and was drunk ...          13  \n",
       "3826   don t know about you but i m soooo over so cal...          16  \n",
       "5660   consume all vs consume all doesn t mention mod...          13  \n",
       "\n",
       "[177 rows x 8 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = pred.reshape((-1,))\n",
    "a = pd.DataFrame(X_test[pred!=y_test])\n",
    "indices = [i for i in a.index]\n",
    "misclassified = twitter_data.iloc[indices,:]\n",
    "misclassified"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
